{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jl67386/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2019-10-04 11:00:22,140 : INFO : loading Word2Vec object from model.w2v\n",
      "2019-10-04 11:00:22,706 : INFO : loading wv recursively from model.w2v.wv.* with mmap=None\n",
      "2019-10-04 11:00:22,707 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-10-04 11:00:22,709 : INFO : loading vocabulary recursively from model.w2v.vocabulary.* with mmap=None\n",
      "2019-10-04 11:00:22,710 : INFO : loading trainables recursively from model.w2v.trainables.* with mmap=None\n",
      "2019-10-04 11:00:22,712 : INFO : setting ignored attribute cum_table to None\n",
      "2019-10-04 11:00:22,714 : INFO : loaded model.w2v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-04 11:00:22,990 : WARNING : From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-04 11:00:23,019 : WARNING : From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-04 11:00:24,947 : WARNING : From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jl67386/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preparation\n",
    "# DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# Set log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Preparation\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preparation\n",
    "# DATASET\n",
    "DATASET_COLUMNS = [\"ids\", 'text', \"target\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TRAIN_SIZE = 0.9\n",
    "\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "# WORD2VEC \n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "# KERAS\n",
    "SEQUENCE_LENGTH = 300\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.3, 0.7)\n",
    "\n",
    "# Import Neural Networks Model\n",
    "KERAS_MODEL = \"model.h5\"\n",
    "WORD2VEC_MODEL = \"model.w2v\"\n",
    "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
    "ENCODER_MODEL = \"encoder.pkl\"\n",
    "\n",
    "w2kmodel = gensim.models.word2vec.Word2Vec.load(WORD2VEC_MODEL)\n",
    "\n",
    "import pickle\n",
    "with open(TOKENIZER_MODEL, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "with open(ENCODER_MODEL, 'rb') as handle:\n",
    "    encoder = pickle.load(handle) \n",
    "    \n",
    "from keras.models import load_model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "## Test Part\n",
    "## Import Dataset\n",
    "boca_sample = pd.read_csv(\"test_10_04.csv\", encoding =DATASET_ENCODING)\n",
    "\n",
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:        \n",
    "        label = NEUTRAL\n",
    "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "            label = NEGATIVE\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = POSITIVE\n",
    "\n",
    "        return label\n",
    "    else:\n",
    "        return NEGATIVE if score < 0.5 else POSITIVE\n",
    "\n",
    "## Data Wrangling & Encoding \n",
    "convert = pad_sequences(tokenizer.texts_to_sequences(boca_sample.text), maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "## Concerning Word Prediction\n",
    "sample_s = model.predict(pad_sequences(convert, maxlen=SEQUENCE_LENGTH))\n",
    "\n",
    "Flags = [decode_sentiment(score, include_neutral=True) for score in sample_s]\n",
    "\n",
    "## Exporting EID\n",
    "out = pd.DataFrame()\n",
    "out['EID'] = boca_sample['ids']\n",
    "out['text'] = boca_sample['text']\n",
    "out['Flag'] = Flags\n",
    "\n",
    "## Import Vader model\n",
    "from nltk.sentiment import vader\n",
    "\n",
    "## from nltk.sentiment import vader\n",
    "nltk.download('vader_lexicon')\n",
    "analysis = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "vader_neg = []\n",
    "for i in range(len(out)):\n",
    "    ans = analysis.polarity_scores(out.iloc[i,1])\n",
    "    if ans['neg'] > 0.1:\n",
    "        vader_neg.append(0)\n",
    "    else:\n",
    "        vader_neg.append(4)\n",
    "        \n",
    "## Add Vader Prediction\n",
    "out['Vader'] = vader_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## High priority -- both models predict as \"WARNING\"\n",
    "EIDs = out[(out['Flag'] == \"NEGATIVE\") & (out['Vader'] == 0)]['EID'].drop_duplicates()\n",
    "EIDs = pd.DataFrame(EIDs)\n",
    "EIDs = EIDs.dropna()\n",
    "EIDs.to_csv(\"EIDs_high_10_04.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lower priority -- Neural Network model predict as \"WARNING\"\n",
    "EIDs = out[(out['Flag'] == \"NEGATIVE\") & (out['Vader'] != 0)]['EID'].drop_duplicates()\n",
    "EIDs = pd.DataFrame(EIDs)\n",
    "EIDs = EIDs.dropna()\n",
    "EIDs.to_csv(\"EIDs_low_10_04.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## High priority -- both models predict as \"WARNING\"\n",
    "EIDs = out[(out['Flag'] == \"NEGATIVE\") & (out['Vader'] == 0)]['EID'].drop_duplicates()\n",
    "EIDs = pd.DataFrame(EIDs)\n",
    "EIDs = EIDs.dropna()\n",
    "EIDs.to_csv(\"EIDs_Q39.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## High priority -- both models predict as \"WARNING\"\n",
    "EIDs = out[(out['Flag'] == \"NEGATIVE\") & (out['Vader'] == 0)]['EID'].drop_duplicates()\n",
    "EIDs = pd.DataFrame(EIDs)\n",
    "EIDs = EIDs.dropna()\n",
    "EIDs.to_csv(\"EIDs_xy_high.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lower priority -- Neural Network model predict as \"WARNING\"\n",
    "EIDs = out[(out['Flag'] == \"NEGATIVE\") & (out['Vader'] != 0)]['EID'].drop_duplicates()\n",
    "EIDs = pd.DataFrame(EIDs)\n",
    "EIDs = EIDs.dropna()\n",
    "EIDs.to_csv(\"EIDs_2_slow.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
